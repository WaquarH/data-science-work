{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "waquar_nlp.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNymk+HB8JbKEA0hKlP3L85",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/WaquarH/data-science-work/blob/main/waquar_nlp.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hZMdQlP-YDGS"
      },
      "source": [
        "<h2>Name - Waquar Haseeb<br>\n",
        "sixth semester<br>\n",
        "NLP Assignment <br>\n",
        "topic:- preprocessing and text document matrix</h2>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zeFO8k1PZJJB"
      },
      "source": [
        "**why we need preprocessing?**<br>\n",
        "Text preprocessing helps us normalize our input data and reduce noises. It could facilitate our analysis; however, improper use of preprocessing could also make us lose important information in our raw data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z24AvA46XPew"
      },
      "source": [
        "#collection of documents\n",
        "para_1 = \"Waquar Haseeb is writing a poem about the river. And eating something spicy\"\n",
        "para_2 = \"i believe that he is not writing instead he is singing \"\n",
        "para_3 = \"i believe that he is shouting instead of singing\"\n",
        "para_4= \"he is just humming\""
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kdA4L6PBa9-O",
        "outputId": "f5b88ced-bb33-4082-e708-44c933307c13"
      },
      "source": [
        "docs = [para_1, para_2, para_3,para_4]\n",
        "docs"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Waquar Haseeb is writing a poem about the river. And eating something spicy',\n",
              " 'i believe that he is not writing instead he is singing ',\n",
              " 'i believe that he is shouting instead of singing',\n",
              " 'he is just humming']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUVoSE3ObSDZ"
      },
      "source": [
        "**Tokenization**<br>\n",
        "A tokenizer splits a sentence into words or a document into sentences. Most of them are implemented by regular expressions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3gBROgLPdg8n",
        "outputId": "c9468336-7ddc-4135-d395-0b118b8a208a"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ym1GQwZhbIVX"
      },
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
        "import string"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ydpOa23Udqf0",
        "outputId": "59217fb5-89b1-4662-9916-6b7f9d52ebe4"
      },
      "source": [
        "for i in docs:\n",
        "  print(i)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Waquar Haseeb is writing a poem about the river. And eating something spicy\n",
            "i believe that he is not writing instead he is singing \n",
            "i believe that he is shouting instead of singing\n",
            "he is just humming\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WmQzuho2bkl9"
      },
      "source": [
        "tokens=[]\n",
        "for i in docs:\n",
        "  for j in (sent_tokenize(i)):\n",
        "    tokens.append(j)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ggti_uLc14n",
        "outputId": "2c27eeb7-6ee7-45a0-b558-6507d0761341"
      },
      "source": [
        "tokens"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Waquar Haseeb is writing a poem about the river.',\n",
              " 'And eating something spicy',\n",
              " 'i believe that he is not writing instead he is singing',\n",
              " 'i believe that he is shouting instead of singing',\n",
              " 'he is just humming']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqKKnN2-d9xU"
      },
      "source": [
        "# remove all punctuations\n",
        "token2=[]\n",
        "trantab = str.maketrans(dict.fromkeys(list(string.punctuation)))\n",
        "for i in tokens:\n",
        "  for j in (word_tokenize(i.translate(trantab))):\n",
        "    token2.append(j) # word level tokens"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SP4fotB-eUaC",
        "outputId": "016f46de-7393-498c-c8e3-bb252ad551e4"
      },
      "source": [
        "token2"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Waquar',\n",
              " 'Haseeb',\n",
              " 'is',\n",
              " 'writing',\n",
              " 'a',\n",
              " 'poem',\n",
              " 'about',\n",
              " 'the',\n",
              " 'river',\n",
              " 'And',\n",
              " 'eating',\n",
              " 'something',\n",
              " 'spicy',\n",
              " 'i',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'he',\n",
              " 'is',\n",
              " 'not',\n",
              " 'writing',\n",
              " 'instead',\n",
              " 'he',\n",
              " 'is',\n",
              " 'singing',\n",
              " 'i',\n",
              " 'believe',\n",
              " 'that',\n",
              " 'he',\n",
              " 'is',\n",
              " 'shouting',\n",
              " 'instead',\n",
              " 'of',\n",
              " 'singing',\n",
              " 'he',\n",
              " 'is',\n",
              " 'just',\n",
              " 'humming']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wY2wcRl5hbH-"
      },
      "source": [
        "**case folding**<br>\n",
        "It is a super easy operation and the purpose is to normalize words into the same form in case (e.g. “The” and “the” will be treated as two different unigrams if no case-folding)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y0yIMR9_hCbB",
        "outputId": "715e992a-af6c-4c1d-8568-fcc4393fd990"
      },
      "source": [
        "# tokenize and lowercase\n",
        "tokens_lowercase=[]\n",
        "for i in token2:\n",
        "  for t in word_tokenize(i):\n",
        "    tokens_lowercase.append(t.lower())\n",
        "print(tokens_lowercase)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['waquar', 'haseeb', 'is', 'writing', 'a', 'poem', 'about', 'the', 'river', 'and', 'eating', 'something', 'spicy', 'i', 'believe', 'that', 'he', 'is', 'not', 'writing', 'instead', 'he', 'is', 'singing', 'i', 'believe', 'that', 'he', 'is', 'shouting', 'instead', 'of', 'singing', 'he', 'is', 'just', 'humming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hUqfhFOmBTLc"
      },
      "source": [
        "**Stopwords filtering**<br>\n",
        "In computing, stop words are words which are filtered out before or after processing of natural language data (text). Though “stop words” usually refers to the most common words in a language, there is no single universal list of stop words used by all natural language processing tools, and indeed not all tools even use such a list"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-acAn7ToiDFw"
      },
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xiMgexMVDudG",
        "outputId": "b6ba08be-904b-4ee0-e45d-6aa29b44bc55"
      },
      "source": [
        "# remove stopwords\n",
        "sw_set = set(stopwords.words('english'))\n",
        "tokens = [t for t in tokens_lowercase if t not in sw_set]\n",
        "print(tokens)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['waquar', 'haseeb', 'writing', 'poem', 'river', 'eating', 'something', 'spicy', 'believe', 'writing', 'instead', 'singing', 'believe', 'shouting', 'instead', 'singing', 'humming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W7MX0O1hFGEQ"
      },
      "source": [
        "**Lemmatization**<br>\n",
        "Lemmatization in linguistics is the process of grouping together the inflected forms of a word so they can be analyzed as a single item, identified by the word’s lemma, or dictionary form. Unlike stemming, lemmatization depends on correctly identifying the intended part of speech and meaning of a word in a sentence, as well as within the larger context surrounding that sentence, such as neighboring sentences or even an entire document.<br>\n",
        "<br>\n",
        "The difference between lemmatization and stemming is that lemmatization utilizes dictionary-like resources to convert a word into its basic form"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nDkWYKnEBC9"
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uaZaDrfhFz_x"
      },
      "source": [
        "# lemmatize by the WordNetLemmatizer\n",
        "wordnet_lemmatizer = WordNetLemmatizer()"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5iksdxMeF4BE",
        "outputId": "8e93e690-7932-4eaa-8369-52c113fbf5e6"
      },
      "source": [
        "print([wordnet_lemmatizer.lemmatize(d, pos='v') for d in tokens]) # POS = verb\n",
        "print([wordnet_lemmatizer.lemmatize(d, pos='a') for d in tokens]) # POS = adjective"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['waquar', 'haseeb', 'write', 'poem', 'river', 'eat', 'something', 'spicy', 'believe', 'write', 'instead', 'sing', 'believe', 'shout', 'instead', 'sing', 'hum']\n",
            "['waquar', 'haseeb', 'writing', 'poem', 'river', 'eating', 'something', 'spicy', 'believe', 'writing', 'instead', 'singing', 'believe', 'shouting', 'instead', 'singing', 'humming']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qHspvh13F9iU"
      },
      "source": [
        "tokens = [wordnet_lemmatizer.lemmatize(d, pos='v') for d in tokens]"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mjBNlR3KGcmg"
      },
      "source": [
        "**Document term Matrix**<br>\n",
        "A document-term matrix is a mathematical matrix that describes the frequency of terms that occur in a collection of documents. In a document-term matrix, rows correspond to documents in the collection and columns correspond to terms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mxr0cLvyGRgU"
      },
      "source": [
        "# Construct a term-document matrix\n",
        "# here as a Python dictionary for ease of interpretability\n",
        "\n",
        "doc_term_matrix = {}\n",
        "\n",
        "for term in tokens:\n",
        "    doc_term_matrix[term] = []\n",
        "    \n",
        "    for doc in tokens:\n",
        "        if term in doc:\n",
        "            doc_term_matrix[term].append(1)\n",
        "        else: doc_term_matrix[term].append(0)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5o0w11EwGlwV",
        "outputId": "6827cb6f-4861-44b9-cc4c-26893c17ee94"
      },
      "source": [
        "doc_term_matrix"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'believe': [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0],\n",
              " 'eat': [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'haseeb': [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'hum': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
              " 'instead': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0],\n",
              " 'poem': [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'river': [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'shout': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0],\n",
              " 'sing': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0],\n",
              " 'something': [0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'spicy': [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'waquar': [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              " 'write': [0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]}"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LS5ptNYVGxxN",
        "outputId": "6d53550a-172d-4599-c694-bf22753e3957"
      },
      "source": [
        "# The query to find all documents containing \"waquar\" AND \"sing\"\n",
        "# Is just a bitwise AND:\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "docs_array = np.array(tokens, dtype='object')\n",
        "\n",
        "v1 = np.array(doc_term_matrix['waquar'])    \n",
        "v2 = np.array(doc_term_matrix['haseeb'])\n",
        "\n",
        "print(v1)\n",
        "print(v2)\n",
        "print('-------')\n",
        "v3 = v1 | v2\n",
        "print(v3)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
            "-------\n",
            "[1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oM2L1JO2HJAM",
        "outputId": "ba76b029-2a10-4389-a72b-adb246ccebfa"
      },
      "source": [
        "# We can now get the matching documents from our corpus with the result\n",
        "[doc for doc in v3 * docs_array if doc]"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['waquar', 'haseeb']"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3Oia2CSZHQod",
        "outputId": "6af9ea23-ad17-463a-aad5-9da9385bf631"
      },
      "source": [
        "v1 = np.array(doc_term_matrix['eat'])    \n",
        "v2 = np.array(doc_term_matrix['spicy'])\n",
        "\n",
        "print(v1)\n",
        "print(v2)\n",
        "print('-------')\n",
        "v3 = v1 | v2\n",
        "print(v3)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
            "[0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
            "-------\n",
            "[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXWWcIGsIGa3",
        "outputId": "f58fa871-3091-484e-c2e2-2fcbb25f8326"
      },
      "source": [
        "[doc for doc in v3 * docs_array if doc]"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['eat', 'spicy']"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euZFesJII-2L"
      },
      "source": [
        "<strong><h3>And doc_term_matrix is the final result of the task.</h3></strong>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVdIsaEQJSV0"
      },
      "source": [
        "Notebook by Waquar Haseeb"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qe2c5zkyITdc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}